---
title: æ¢¯åº¦ä¸‹é™æ‹Ÿåˆæ›²çº¿
date: 2023-02-01 00:00:08
tags: 
  - gradient descent
  - least square method

---

# æ¢¯åº¦ä¸‹é™æ‹Ÿåˆæ›²çº¿

åŸæœ¬æ‰“ç®—ä½¿ç”¨pythonä»£ç å®ç°æ¢¯åº¦ä¸‹é™æ›²çº¿æ‹Ÿåˆã€‚æœç´¢äº†ä¸€åœˆæ²¡æœ‰æ‰¾åˆ°è®²å¾—éå¸¸è¯¦ç»†çš„æ–‡ç« ã€‚è‡ªå·±å®è·µäº†ä¸€ä¸‹è®°å½•åœ¨æ­¤ã€‚

æ³¨ï¼šå…¬å¼æ˜¯åæœŸæ•´ç†çš„æ²¡æœ‰å¾ˆä»”ç»†ï¼Œä¹Ÿè®¸ä¼šæœ‰äº›ç»†èŠ‚ä¸Šçš„é—®é¢˜ï¼Œä»…ä¾›å‚è€ƒã€‚

## åŸç†

é¦–å…ˆç¡®å®šæ‹Ÿåˆæ›²çº¿çš„å‡½æ•°æ˜¯æ€æ ·çš„ï¼Œè¿™é‡Œç”¨ä¸€å…ƒkæ¬¡æ–¹ç¨‹ï¼š

$y(w_0,w_1...w_k) = w_0 + w_1 x + w_2 x^2 ... + w_k x^k = \sum_{n=0}^{k} {w_n}{x^n}$

ç„¶åï¼Œç¡®å®šä½¿ç”¨æœ€å°äºŒä¹˜æŸå¤±å‡½æ•°ï¼š

$L = \sum_{i=0}^{j} \frac{1}{2j} (\hat{y_i} - ğ‘¦_i) ^ 2$

è¿™é‡Œçš„$\hat{y}$æ˜¯è§‚æµ‹å€¼, $y_i$æ˜¯ç†è®ºå€¼(ground truth)ï¼Œ$j$æ˜¯ä¸€æ¬¡è¿­ä»£çš„æ ·æœ¬ä¸ªæ•°ã€‚

æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ï¼Œå°±æ˜¯å¯¹losså‡½æ•°çš„æ¯ä¸€ä¸ªwæƒé‡åˆ†é‡åˆ†åˆ«è®¡ç®—æ¢¯åº¦ï¼Œç„¶åæ¯æ¬¡è¿­ä»£æ›´æ–°ä¸€æ¬¡æ¢¯åº¦ã€‚

å¯¹äºæ¯ä¸ªwæƒé‡çš„æ¢¯åº¦ï¼Œä¹Ÿå°±æ˜¯æ¯ä¸ª$w_g$çš„åå¯¼æ•°ï¼Œè®¡ç®—å…¬å¼æ˜¯ï¼š$\frac{d}{dw_g}L = \frac{d}{dw_g}\sum_{i=0}^{j} \frac{1}{2j} (\hat{y_i} - ğ‘¦_i) ^ 2$

è¿™é‡Œä»¥$k = 2$, gä¸º1å’Œ2çš„æƒ…å†µä¸ºä¾‹å±•å¼€æ¨å¯¼ï¼Œé¦–å…ˆç®—å‡º$(\hat{y} - y)^2 = $

$( w_0^2 + w_0w_1x + w0w_2^2 - w_0y$

$+ w_0w_1x + w_1^2x^2 + w_1w_2x^3 - w_1xy$

$+ w_0w_2x^2 + w_1w_2x^3 + w_2^2x^4 - w_2x^2y$

$- w_0y - w_1xy - w_2x^2y + y^2)$

å¯¹$w_1$æ±‚å¯¼å, å¾—: 

$= (w_0x + w_0x + 2w_1x^2 + w_2x^3 - xy + w_2x^3 - xy)$

$= (2w_0x + 2w_1x^2 + 2w_2x^3 - 2xy)$

$= 2x(w_0 + w_1x + w_2x^2 - y)$

å¯¹$w_2$æ±‚å¯¼å, å¾—: 

$= 2x^2(w_0 + w_1x + w_2x^2 - y)$

æ‰€ä»¥ $\frac{d}{dw_g}L = x^g(\sum_{n=0}^{k} {w_n}{x^n} - y)$

å¯¹äºæ¯æ¬¡è¿­ä»£æƒé‡$w_k$çš„æ¢¯åº¦ä¸º:

$gradient(w_g) = \sum_{i=1}^{j} \frac{1}{j} x^g(\sum_{n=0}^{k} {w_n}{x^n} - y)$

ä¸‹é¢å°±æ˜¯æƒé‡æ›´æ–°çš„å…¬å¼ï¼š

$w_k = w_k - \epsilon \frac{dL}{d w_g}$

å…¶ä¸­ $\epsilon$ ä¸ºå­¦ä¹ ç‡ã€‚

## æµ‹è¯•æ•°æ®

è¿™é‡Œè®¾è¦æ‹Ÿåˆä¸€ä¸ªæ¥è¿‘ä¸€å…ƒä¸‰æ¬¡å‡½æ•°$f(x) = ax^3+bx^2+cx+d$åˆ†å¸ƒçš„æ•£ç‚¹çš„æ›²çº¿ï¼Œç”Ÿæˆæ ·æœ¬æ•°æ®ã€‚

```python
import math
import numpy as np
import matplotlib.pyplot as plt

# -3x^3 + 2x^2 + -4x + 2
ground_truth_w = [-3, 2, -4, 2]
ground_truth_x = np.arange(-5, 4, 0.5)
ground_truth_y = np.polyval(ground_truth_w, ground_truth_x)
# åŠ å™ªéŸ³
ground_truth_y_noise = [y + np.random.randn() * 30 for y in ground_truth_y]
```

![Sample](/img/gd_sample.png)

ä»¥ä¸Šæ•£ç‚¹æ˜¯è¦æ‹Ÿåˆçš„æ›²çº¿çš„è¾“å…¥æ ·æœ¬ã€‚

## æ‹Ÿåˆä»£ç 

```python
import math
import numpy as np
import matplotlib.pyplot as plt

# æ²¡åšSGD, æ¯æ¬¡ä½¿ç”¨æ‰€æœ‰æ ·æœ¬è®¡ç®—æ¢¯åº¦
batch_size = len(ground_truth_y)
# å­¦ä¹ ç‡
eps = [0.0001, 0.0001, 0.0001]
# éšæœºåˆå§‹åŒ–ç³»æ•°æƒé‡, æƒé‡ä¸ªæ•°-1å³ä¸ºNæ¬¡æ–¹çš„N
# weights = [np.random.randn(4), np.random.randn(3), np.random.randn(2)]
weights = [np.zeros(4), np.zeros(3), np.zeros(2)]
# è¿­ä»£æ¬¡æ•°
epochs = [2000, 2000, 2000]

def evaluate_loss(x, y, weights):
    s = 0
    bs = len(y)
    for i in range(bs):
        s = s + (np.polyval(weights, x[i]) - y[i]) ** 2
    return s / bs;

def evaluate_gradient(x, y, weights, epsilon):
    power = len(weights)
    for g in range(len(weights)):
        gradient = 0
        for i in range(batch_size):
            gradient += x[i] ** (power - 1 - g) * (np.polyval(weights, x[i]) - y[i]) / batch_size
        weights[g] = weights[g] - epsilon * gradient
        
def fit_curve(x, y, weights, epochs, epsilon):
    for i in range(epochs):
        evaluate_gradient(x, y, weights, epsilon)
        if i % 100 == 0:
            loss = evaluate_loss(x, y, weights)
            print(f'epoch:{i:8} loss:{loss:.4f}')

for i in range(len(weights)):
    fit_curve(ground_truth_x, ground_truth_y_noise, weights[i], epochs[i], eps[i])

```

è¾“å‡ºå›¾åƒï¼š

![Result](/img/gd_curvefit.png)

## å¸¸è§é—®é¢˜

### è®­ç»ƒå‡ºç°nan

å­¦ä¹ ç‡å¤ªå¤§, æ ·æœ¬æ•°æ®yå€¼å¤ªå¤§

### lossé™ä¸ä¸‹æ¥

lossä¸‹é™å¾ˆå°è¯´æ˜ä¼˜åŒ–ç©ºé—´å·²ç»ä¸å¤§äº†, ä¸ä¼šé™åˆ°1ä»¥ä¸‹çš„

### æ‹Ÿåˆä¸æ”¶æ•›

å¯èƒ½ä¹‹ä¸€æ˜¯gradientç®—é”™äº†ï¼Œæ¯”å¦‚`x[i]`çš„æŒ‡æ•°ç®—é”™ï¼Œæˆ–è€…weightsçš„é¡ºåºæ²¡å¯¹åº”ä¸Šä¹‹ç±»ã€‚
